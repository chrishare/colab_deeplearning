{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conv_autoencoder_mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrishare/colab_deeplearning/blob/master/conv_autoencoder_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TgqMU8iSXD0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "5a71f779-6282-47b2-9284-fcdc80b1643e"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This listing will build a convolutional autoencoder on the MNIST dataset,\n",
        "with code taken from https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "\n",
        "The purpose of this listing is to explain exactly what the code is doing. These\n",
        "are just notes for myself based on my current understanding. Hopefully I can\n",
        "come back to these later when I better understand what is happening.\n",
        "\n",
        "We really want to run this on a GPU. It is going to run 5x times faster than\n",
        "on CPU, atleast when run on Colab. I think this is because the number of \n",
        "operations that can be accelerated on the GPU here are significant, including\n",
        "the convolution operations.\n",
        "\n",
        "We're using Keras so we need various layers:\n",
        "Input - The input features (just 1 of these)\n",
        "Dense - Dense, fully-connected layers\n",
        "Conv2D - Overlaying our image matrix (for a channel) with a convolutional\n",
        "kernel, and then performing elementwise multiplication and summing to get the\n",
        "result. We repeat this many times by sliding the kernel across the input matrix\n",
        "abd end up with an output matrix for further processing. We don't necessarily \n",
        "slide (stride) the full length or width of the kernel.\n",
        "We do this to efficiently transform (sharpen, blur, do edge detection,\n",
        "etc) the feautrespace before doing regular neural net processing. Some common\n",
        "kernels are here: https://en.wikipedia.org/wiki/Kernel_(image_processing).\n",
        "MaxPooling2D - Similar to Conv2D, but takes the maximum value of the\n",
        "intersection between the two areas. Usually, we will stride the width/length\n",
        "of the pooling kernel, so that we'll never overlap. This is because the purpose\n",
        "of this operation is basically to downsample the image. This gives us a few\n",
        "benefits - it reduces computation costs by reducing dimensionality, and also\n",
        "helps prevent overfitting by 'abstracting' out low-level details in the input.\n",
        "UpSampling2D - Achieves basically the reverse of what MaxPooling2D does.\n",
        "\n",
        "Of these layers, MaxPooling2D and UpSampling2D are not trained - they don't \n",
        "have parameters or weights, they just perform the operation they need to.\n",
        "Conv2D layers are indeed trained. The kernel values are trained such that the\n",
        "NN can determine what kind of convolution is best - what kind of edges and\n",
        "shapes are valuable, and so on.\n",
        "\n",
        "You'll see two different kinds of padding:\n",
        "Same = Add zeros to the side of the input to make it big enough to perform\n",
        "the final convolutions.\n",
        "Valid - Don't pad - just drop the final operation / elements.\n",
        "\n",
        "Another argument to the Conv2d etc layers is the number of filters. This arg,\n",
        "say 10, means we will have 10 'channels' or stacked filters, that are trained\n",
        "independantly - this means we can run many kinds of shape detector / transform\n",
        "on the same image / input. It will also mean the output has 10 x dimensions.\n",
        "\n",
        "We may want to perform stacks of convolution and pooling operations - not just\n",
        "one. This is thought to be biologically plausible, at least in that many layers\n",
        "of low level operations seem to run as preprocessing before richer processing\n",
        "is done.\n",
        "\n",
        "We also want to import the Keras backend in order to use backend e.g.\n",
        "Tensorflow primitives (operations) directly, such as mean and exp.\n",
        "\"\"\"\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "# MNIST is 28x28 with just one channel. Channels are usually for colour but\n",
        "# can also represent other kinds of data, like heatmap information.\n",
        "input_img = Input(shape=(28, 28, 1))  \n",
        "\n",
        "\"\"\" We define first our NN that goes from the input image to the encoded\n",
        "representation. Generally we are using rectified linear units as the activation\n",
        "but use sigmoid on the final layer decoder layer to get back to a probability\n",
        "estimate.\n",
        "\"\"\"\n",
        "\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "print(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "print(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "print(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "print(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "print(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "print(encoded)\n",
        "\n",
        "\"\"\" You will get these shapes.\n",
        "Tensor(\"conv2d_29/Relu:0\", shape=(?, 28, 28, 16), dtype=float32)\n",
        "Tensor(\"max_pooling2d_13/MaxPool:0\", shape=(?, 14, 14, 16), dtype=float32)\n",
        "Tensor(\"conv2d_30/Relu:0\", shape=(?, 14, 14, 8), dtype=float32)\n",
        "Tensor(\"max_pooling2d_14/MaxPool:0\", shape=(?, 7, 7, 8), dtype=float32)\n",
        "Tensor(\"conv2d_31/Relu:0\", shape=(?, 7, 7, 8), dtype=float32)\n",
        "Tensor(\"max_pooling2d_15/MaxPool:0\", shape=(?, 4, 4, 8), dtype=float32)\n",
        "\n",
        "The encoded layer has 4 * 4 * 8 = 128 dimensions, but that doesn't mean that\n",
        "all of those dimensions will be used, especially if we penalise activation.\n",
        "\n",
        "Encoded represents our latent space. It's our 'compressed' feature space.\n",
        "At this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "We now have to reverse the process back up to a digit.\n",
        "\n",
        "I have read that it is a good idea to to tie weights in equivalent layers from\n",
        "the encode and decode halves. That is, we tell Keras etc that the encode and \n",
        "decode layers will share weights with their counterparts. This involves taking \n",
        "a transpose of the weights from the encode layer and using them in the decoder.\n",
        "This means fewer parameters to learn, making it faster and less likely to \n",
        "overfit. We don't take this approach here. You can see information here:\n",
        "https://amiralavi.net/blog/2018/08/25/tied-autoencoders\n",
        "\"\"\"\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "print(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "print(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "print(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "print(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "print(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "print(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "print(decoded)\n",
        "\n",
        "\"\"\" You will get these shapes:\n",
        "\n",
        "Tensor(\"up_sampling2d_16/ResizeNearestNeighbor:0\", shape=(?, 8, 8, 8), dtype=float32)\n",
        "Tensor(\"conv2d_40/Relu:0\", shape=(?, 8, 8, 8), dtype=float32)\n",
        "Tensor(\"up_sampling2d_17/ResizeNearestNeighbor:0\", shape=(?, 16, 16, 8), dtype=float32)\n",
        "Tensor(\"conv2d_41/Relu:0\", shape=(?, 14, 14, 16), dtype=float32)\n",
        "Tensor(\"up_sampling2d_18/ResizeNearestNeighbor:0\", shape=(?, 28, 28, 16), dtype=float32)\n",
        "Tensor(\"conv2d_42/Sigmoid:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
        "\n",
        "Finally, let's construct a model with these layers and optomize on the entropy\n",
        "loss between the original and the reconstruction.\n",
        "\"\"\"\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"conv2d_43/Relu:0\", shape=(?, 28, 28, 16), dtype=float32)\n",
            "Tensor(\"max_pooling2d_19/MaxPool:0\", shape=(?, 14, 14, 16), dtype=float32)\n",
            "Tensor(\"conv2d_44/Relu:0\", shape=(?, 14, 14, 8), dtype=float32)\n",
            "Tensor(\"max_pooling2d_20/MaxPool:0\", shape=(?, 7, 7, 8), dtype=float32)\n",
            "Tensor(\"conv2d_45/Relu:0\", shape=(?, 7, 7, 8), dtype=float32)\n",
            "Tensor(\"max_pooling2d_21/MaxPool:0\", shape=(?, 4, 4, 8), dtype=float32)\n",
            "Tensor(\"conv2d_46/Relu:0\", shape=(?, 4, 4, 8), dtype=float32)\n",
            "Tensor(\"up_sampling2d_19/ResizeNearestNeighbor:0\", shape=(?, 8, 8, 8), dtype=float32)\n",
            "Tensor(\"conv2d_47/Relu:0\", shape=(?, 8, 8, 8), dtype=float32)\n",
            "Tensor(\"up_sampling2d_20/ResizeNearestNeighbor:0\", shape=(?, 16, 16, 8), dtype=float32)\n",
            "Tensor(\"conv2d_48/Relu:0\", shape=(?, 14, 14, 16), dtype=float32)\n",
            "Tensor(\"up_sampling2d_21/ResizeNearestNeighbor:0\", shape=(?, 28, 28, 16), dtype=float32)\n",
            "Tensor(\"conv2d_49/Sigmoid:0\", shape=(?, 28, 28, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g4SILJ-xYdif",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's import the MNIST dataset, which will download from the net\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "# Extract the data. Here, we do not care about the labels.\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "# Scale to get a 0..1 float of pixel intensity. No 0 centreing or norm here.\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Reshape to be 28*28*1 instead of 28*28, as required by our layers.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  \n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qyXGNIlbYh7l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        },
        "outputId": "88051760-3559-4e7c-bd7a-2cb12be5e0e2"
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=50,\n",
        "                batch_size=128,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.2225 - val_loss: 0.1755\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1586 - val_loss: 0.1489\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1414 - val_loss: 0.1312\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1321 - val_loss: 0.1276\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1266 - val_loss: 0.1261\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1228 - val_loss: 0.1216\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1199 - val_loss: 0.1165\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1172 - val_loss: 0.1165\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1157 - val_loss: 0.1113\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1138 - val_loss: 0.1118\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1126 - val_loss: 0.1093\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1116 - val_loss: 0.1115\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1106 - val_loss: 0.1062\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1097 - val_loss: 0.1069\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1089 - val_loss: 0.1106\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1079 - val_loss: 0.1060\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1075 - val_loss: 0.1045\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1067 - val_loss: 0.1064\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1061 - val_loss: 0.1044\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1056 - val_loss: 0.1038\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1052 - val_loss: 0.1051\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1049 - val_loss: 0.1031\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1045 - val_loss: 0.1050\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1043 - val_loss: 0.1006\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1038 - val_loss: 0.1049\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1035 - val_loss: 0.1020\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1032 - val_loss: 0.1016\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1030 - val_loss: 0.1007\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1026 - val_loss: 0.1013\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1025 - val_loss: 0.1063\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1023 - val_loss: 0.0980\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1018 - val_loss: 0.0975\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1018 - val_loss: 0.0977\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1014 - val_loss: 0.0989\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.1013 - val_loss: 0.1014\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.1011 - val_loss: 0.1018\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1010 - val_loss: 0.0980\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1008 - val_loss: 0.1021\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1008 - val_loss: 0.0993\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1008 - val_loss: 0.1000\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1003 - val_loss: 0.0967\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.0999 - val_loss: 0.0982\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0998 - val_loss: 0.1012\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.1000 - val_loss: 0.1016\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.1000 - val_loss: 0.0990\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.0997 - val_loss: 0.0980\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.0994 - val_loss: 0.0973\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0993 - val_loss: 0.0993\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.0993 - val_loss: 0.1010\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.0992 - val_loss: 0.0992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f94931ff898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "l3W7mf1xYty7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "6e2964a3-7cc9-4c01-99c7-4c6ca90ba374"
      },
      "cell_type": "code",
      "source": [
        "# Import matplotlib for data visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "# We're going to print 2 rows, 10 columns of images - the top row will have \n",
        "# originals and the bottom will have the reconstruction\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + n + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    # Don't show axes \n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    # Don't show axes \n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "# Actually show the plot\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAADjCAYAAADdR/IFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f94912c2eb8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "II9r1mS-apvG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4avKvBUs5oUz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}