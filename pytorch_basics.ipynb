{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGrrzYc1s40MSvhiEkvLEv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrishare/colab_deeplearning/blob/master/pytorch_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK2qy-3OEPuc",
        "outputId": "e5e08a2a-cf0c-4955-b1fa-bd20eac67b1a"
      },
      "source": [
        "# Some pytorch basics - first, a simple numpy-based regression problem\n",
        "# that attempts to produce learn a,b,c,d such that a + bx + cx^2 + dx^3\n",
        "# closely models sin(x) between -pi and +pi\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Create random input and output data\n",
        "# Get a row vector of linearly spaced numbers between -pi and +pi\n",
        "x = np.linspace(-math.pi, math.pi, 2000)\n",
        "# Get a row vector of sin(x_i) for each i in x\n",
        "y = np.sin(x)\n",
        "# Get the 'size'of x - which will be (2000,) - a 1-dim row vector of size 2000\n",
        "x.shape\n",
        "\n",
        "# Randomly initialize weights - get 4 individual floats that are \n",
        "a = np.random.randn()\n",
        "b = np.random.randn()\n",
        "c = np.random.randn()\n",
        "d = np.random.randn()\n",
        "\n",
        "# Set learning rate to be 1 / 10^6\n",
        "learning_rate = 1e-6\n",
        "\n",
        "# For 2000 iterations (epochs)\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    # y = a + b x + c x^2 + d x^3\n",
        "    # y is a 2000-el row vector based on the current weights\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss by calculating the square of every difference \n",
        "    # between the predication and actual answer (label), and summing\n",
        "    # So loss is a scalar of the magnitude of the loss on the whole dataset\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    # a = sum(2 * (y_pred - y_actual)) for each el in y\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    # b = sum(2 * (y_pred - y_actual) * x) for each el in y\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    # c = sum(2 * (y_pred - y_actual) * x^2) for each el in y\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    # d = sum(2 * (y_pred - y_actual) * x^3) for each el in y\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # Update weights - subtrack the gradient of the loss * learning rate\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 18.09889106562996\n",
            "199 15.105687454472273\n",
            "299 13.082607103470345\n",
            "399 11.713518430560395\n",
            "499 10.786040027133247\n",
            "599 10.157057297824208\n",
            "699 9.730041053359837\n",
            "799 9.439820602584472\n",
            "899 9.242353029135224\n",
            "999 9.10784353510595\n",
            "1099 9.0161151779539\n",
            "1199 8.95348972519423\n",
            "1299 8.910684511709542\n",
            "1399 8.881393000359349\n",
            "1499 8.86132584235104\n",
            "1599 8.847562388040501\n",
            "1699 8.838111690051994\n",
            "1799 8.831615010272614\n",
            "1899 8.827144003023928\n",
            "1999 8.824063654947992\n",
            "Result: y = 0.0022636170507037193 + 0.8582230470700756 x + -0.0003905116079966977 x^2 + -0.09354121965192531 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8puQWXjnEwhN"
      },
      "source": [
        "# Next, a pytorch example - though it uses CPU\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# Get a reference to the torch float datatype\n",
        "dtype = torch.float\n",
        "# Get a reference to the local CPU device - you can get a cuda GPU too\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# Create random input and output data, on the CPU using torch.float\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "# Get the ground truth sin(x) for every input x\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Randomly initialize weights, again, on CPU using torch.floar\n",
        "a = torch.randn((), device=device, dtype=dtype)\n",
        "b = torch.randn((), device=device, dtype=dtype)\n",
        "c = torch.randn((), device=device, dtype=dtype)\n",
        "d = torch.randn((), device=device, dtype=dtype)\n",
        "\n",
        "# The rest is as per numpy - it magically does the computation on the torch\n",
        "# device and types acquired when setting up the variables/tensors\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF_J7LdQ6aEy",
        "outputId": "8cf6a555-fa60-4597-bb9a-2798301f9fbd"
      },
      "source": [
        "# Finally, use pytorch functions to shorten the program and use out of the box\n",
        "# optimisation\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Prepare the input tensor (x, x^2, x^3).\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "# Use the nn package to define our model and loss function.\n",
        "model = torch.nn.Sequential(\n",
        "    # 3 input features, 1 output feature - a linear transform xA + b\n",
        "    torch.nn.Linear(3, 1),\n",
        "    # Flatten dims 0 and 1 into a tensor\n",
        "    torch.nn.Flatten(0, 1)\n",
        ")\n",
        "\n",
        "# Define a lose function using mean squared error, and sum the output\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "# Use the optim package to define an Optimizer that will update the weights of\n",
        "# the model for us. Here we will use RMSprop; the optim package contains many other\n",
        "# optimization algorithms. The first argument to the RMSprop constructor tells the\n",
        "# optimizer which Tensors it should update.\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "# For 2000 iterations over the dataset (2000 epochs)\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y by passing x to the model.\n",
        "    y_pred = model(xx)\n",
        "\n",
        "    # Compute and print loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Before the backward pass, use the optimizer object to zero all of the\n",
        "    # gradients for the variables it will update (which are the learnable\n",
        "    # weights of the model). This is because by default, gradients are\n",
        "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to model\n",
        "    # parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Calling the step function on an Optimizer makes an update to its\n",
        "    # parameters\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "linear_layer = model[0]\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 25854.71484375\n",
            "199 12355.208984375\n",
            "299 6130.99951171875\n",
            "399 3682.5263671875\n",
            "499 2915.4697265625\n",
            "599 2559.28125\n",
            "699 2241.237060546875\n",
            "799 1938.8804931640625\n",
            "899 1663.5596923828125\n",
            "999 1417.183837890625\n",
            "1099 1196.697509765625\n",
            "1199 998.4860229492188\n",
            "1299 820.563720703125\n",
            "1399 662.1236572265625\n",
            "1499 522.6375732421875\n",
            "1599 401.6993713378906\n",
            "1699 299.1456604003906\n",
            "1799 213.55767822265625\n",
            "1899 145.0023956298828\n",
            "1999 92.24667358398438\n",
            "Result: y = -0.0008279461762867868 + 0.5763206481933594 x + -0.0008301403722725809 x^2 + -0.05408855527639389 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CahZl5yG99tP",
        "outputId": "944f3ce7-e7ff-456c-fc7d-d53e97080ce1"
      },
      "source": [
        "import torch, torch.nn\n",
        "import math\n",
        "m = torch.nn.Linear(4, 5)\n",
        "print(m)\n",
        "print(m.weight)\n",
        "print(m.bias)\n",
        "input = torch.randn(10, 4)\n",
        "print(input)\n",
        "output = m(input)\n",
        "print(output)\n",
        "print(output.size())\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=4, out_features=5, bias=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.3601, -0.3429, -0.0460, -0.3918],\n",
            "        [-0.3225,  0.3899, -0.0995,  0.2800],\n",
            "        [-0.2407, -0.1779,  0.4001,  0.4539],\n",
            "        [ 0.1469, -0.2761, -0.0304, -0.3208],\n",
            "        [ 0.4796, -0.1443,  0.3001, -0.1271]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.2080,  0.0932,  0.3847, -0.1223,  0.2236], requires_grad=True)\n",
            "tensor([[ 0.8187, -0.3245,  0.4071, -1.1526],\n",
            "        [ 0.0976,  0.7363,  0.4647, -0.2185],\n",
            "        [-0.9918, -0.2333,  0.5036,  1.8562],\n",
            "        [-0.6338,  1.0718, -0.7214, -0.3042],\n",
            "        [ 0.3224, -1.9709,  0.6342,  1.0381],\n",
            "        [ 0.3462, -0.7425, -1.0589, -0.7979],\n",
            "        [-0.3125, -0.8376,  0.9666,  0.0416],\n",
            "        [-1.1291, -1.3341,  0.7975, -0.7326],\n",
            "        [-0.3605,  0.5124, -0.6584,  0.7430],\n",
            "        [ 0.4290,  1.5329, -0.5346,  0.2783]])\n",
            "tensor([[ 0.6310, -0.6606, -0.1150,  0.4450,  0.9318],\n",
            "        [-0.3611,  0.2414,  0.3170, -0.2553,  0.3314],\n",
            "        [-1.2356,  0.7918,  1.7090, -0.8144, -0.3032],\n",
            "        [-0.6514,  0.7021, -0.0801, -0.3918, -0.4129],\n",
            "        [ 0.1481, -0.5516,  1.3826,  0.1170,  0.7212],\n",
            "        [ 0.5327, -0.4260, -0.3523,  0.4218,  0.2805],\n",
            "        [-0.0941, -0.2171,  1.0145,  0.0203,  0.4795],\n",
            "        [ 0.0933, -0.3473,  0.8803,  0.2909,  0.2071],\n",
            "        [-0.7743,  0.6828,  0.4542, -0.5351, -0.3153],\n",
            "        [-0.6637,  0.6837, -0.0788, -0.5555,  0.0123]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "torch.Size([10, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct9HtMIVHSEi",
        "outputId": "7ac26383-0c50-4d3b-fdbe-6c3e5b7bb4b9"
      },
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi.git\n",
        "%cd cocoapi/PythonAPI\n",
        "!pwd\n",
        "!ls ./cocoapi/PythonAPI\n",
        "!python setup.py build_ext install"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'cocoapi' already exists and is not an empty directory.\n",
            "/content/cocoapi/PythonAPI\n",
            "/content/cocoapi/PythonAPI\n",
            "ls: cannot access './cocoapi/PythonAPI': No such file or directory\n",
            "running build_ext\n",
            "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
            "/usr/local/lib/python3.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "building 'pycocotools._mask' extension\n",
            "creating build\n",
            "creating build/common\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.7/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
            "       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
            "                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
            "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
            "                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
            "   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
            "   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
            "                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
            "     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
            "                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToBbox\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:141:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kxp\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "       if(j%2==0) xp=x; else if\u001b[01;35m\u001b[K(\u001b[m\u001b[Kxp<x) { ys=0; ye=h-1; }\n",
            "                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/../common/maskApi.o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -o build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating pycocotools.egg-info\n",
            "writing pycocotools.egg-info/PKG-INFO\n",
            "writing dependency_links to pycocotools.egg-info/dependency_links.txt\n",
            "writing requirements to pycocotools.egg-info/requires.txt\n",
            "writing top-level names to pycocotools.egg-info/top_level.txt\n",
            "writing manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
            "writing manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "copying pycocotools/__init__.py -> build/lib.linux-x86_64-3.7/pycocotools\n",
            "copying pycocotools/mask.py -> build/lib.linux-x86_64-3.7/pycocotools\n",
            "copying pycocotools/coco.py -> build/lib.linux-x86_64-3.7/pycocotools\n",
            "copying pycocotools/cocoeval.py -> build/lib.linux-x86_64-3.7/pycocotools\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/pycocotools\n",
            "copying build/lib.linux-x86_64-3.7/pycocotools/__init__.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
            "copying build/lib.linux-x86_64-3.7/pycocotools/mask.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
            "copying build/lib.linux-x86_64-3.7/pycocotools/coco.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
            "copying build/lib.linux-x86_64-3.7/pycocotools/cocoeval.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
            "copying build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg/pycocotools\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/mask.py to mask.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/coco.py to coco.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/cocoeval.py to cocoeval.cpython-37.pyc\n",
            "creating stub loader for pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/_mask.py to _mask.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocotools.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocotools.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocotools.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocotools.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocotools.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "pycocotools.__pycache__._mask.cpython-37: module references __file__\n",
            "creating dist\n",
            "creating 'dist/pycocotools-2.0-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing pycocotools-2.0-py3.7-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.7/dist-packages/pycocotools-2.0-py3.7-linux-x86_64.egg\n",
            "Extracting pycocotools-2.0-py3.7-linux-x86_64.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding pycocotools 2.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/pycocotools-2.0-py3.7-linux-x86_64.egg\n",
            "Processing dependencies for pycocotools==2.0\n",
            "Searching for matplotlib==3.2.2\n",
            "Best match: matplotlib 3.2.2\n",
            "Adding matplotlib 3.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Cython==0.29.22\n",
            "Best match: Cython 0.29.22\n",
            "Adding Cython 0.29.22 to easy-install.pth file\n",
            "Installing cygdb script to /usr/local/bin\n",
            "Installing cython script to /usr/local/bin\n",
            "Installing cythonize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for setuptools==56.0.0\n",
            "Best match: setuptools 56.0.0\n",
            "Adding setuptools 56.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for kiwisolver==1.3.1\n",
            "Best match: kiwisolver 1.3.1\n",
            "Adding kiwisolver 1.3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for cycler==0.10.0\n",
            "Best match: cycler 0.10.0\n",
            "Adding cycler 0.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for python-dateutil==2.8.1\n",
            "Best match: python-dateutil 2.8.1\n",
            "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pyparsing==2.4.7\n",
            "Best match: pyparsing 2.4.7\n",
            "Adding pyparsing 2.4.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for pycocotools==2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zuS2CFT69_jU",
        "outputId": "6049ce5f-0d38-4e84-cfd1-3317f2191704"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "torch.manual_seed(42)\n",
        "use_cuda = False\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_kwargs = {'batch_size': 64}\n",
        "test_kwargs = {'batch_size': 100 }\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {'num_workers': 1,\n",
        "                    'pin_memory': True,\n",
        "                    'shuffle': True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                    transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.1)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "for epoch in range(1, 14 + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313800\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.013504\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.445841\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.962768\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.801057\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.795760\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.565556\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.519057\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.798186\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.475240\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.302890\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.492607\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.532345\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.456702\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.379095\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.385735\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.491051\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.322088\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.736985\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.411895\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.435325\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.287178\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.474649\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.673634\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.288116\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.654488\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.454379\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.273499\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.234038\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.304111\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.376909\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.220612\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.171101\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.319789\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.066549\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.212818\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.345118\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.541009\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.120517\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.283285\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.217470\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.215678\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.363865\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.194223\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.290945\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.164262\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.231306\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.299998\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.371168\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.248862\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.242977\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.262471\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.278233\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.107036\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.141026\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.237739\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.247212\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.125645\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.259334\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.218259\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.136946\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.082794\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.165881\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.255509\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.218711\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.214976\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.115071\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.243768\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.155940\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.093574\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.214545\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.311100\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.373934\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.214843\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.160962\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.142528\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.110707\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.048528\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.071316\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.189833\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.235912\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.097335\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.031148\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.195974\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.144287\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.112662\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.085557\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.219281\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.132114\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.120541\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.301605\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.039948\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.045991\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.018111\n",
            "\n",
            "Test set: Average loss: 0.0869, Accuracy: 9740/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.086137\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.057867\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.120412\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.115402\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.080727\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.086961\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.046166\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.136707\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.152502\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.159424\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.213298\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.121853\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.089218\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.061947\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.181920\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.081868\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.317773\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.031478\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.342888\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.179497\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.107033\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.099453\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.135737\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.192894\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.178638\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.135439\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.168284\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.042356\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.116121\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.215107\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.195103\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.109493\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.067357\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.083312\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.031188\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.069735\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.176452\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.214676\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.028359\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.072774\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.127100\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.051594\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.249924\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.112942\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.076635\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.140211\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.070320\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.109420\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.100199\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.116439\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.180734\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.157482\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.193627\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.031986\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.099592\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.120727\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.152143\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.167551\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.123026\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.139889\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.094721\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.038686\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.077869\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.067116\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.119684\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.128358\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.036178\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.128706\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.144743\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.046056\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.176453\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.200305\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.150888\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.122062\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.096425\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.105856\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.081495\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.038048\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.122812\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.117864\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.139537\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.078598\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.066402\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.194408\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.076116\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.029791\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.041815\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.124309\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.035995\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.056319\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.210739\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.014573\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.007965\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.003667\n",
            "\n",
            "Test set: Average loss: 0.0590, Accuracy: 9804/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.087846\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.089046\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.082622\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.169127\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.093844\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.062418\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.041903\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.095217\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.185099\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.095539\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.145372\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.103776\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.068722\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.038038\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.116124\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.113928\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.149375\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.039944\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.205806\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.125071\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.117936\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.041722\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.070317\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.107706\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.048907\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.159589\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.192310\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.021775\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.104661\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.116642\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.158799\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.098155\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.022527\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.025364\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.023836\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.058288\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.063327\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.126117\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.010350\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.031537\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.095247\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.041797\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.206195\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.107162\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.055827\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.032289\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.050976\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.065025\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.060198\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.102527\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.118056\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.128386\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.081553\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.011800\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.070407\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.186298\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.178253\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.057214\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.047615\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.180592\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.134607\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.014881\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.059592\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.085660\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.057986\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.070216\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.070635\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.128288\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.076372\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.054892\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.167316\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.172834\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.160821\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.126267\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.067229\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.034407\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.122775\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.017910\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.034068\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.073192\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.137530\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.085061\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.038115\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.183880\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.132894\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.073626\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.052733\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.138822\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.059591\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.055806\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.163860\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.028339\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.019194\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.005360\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 9824/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.027468\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.053219\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.091862\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.152258\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.075492\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.043896\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.051998\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.087874\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.136091\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.058619\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.084298\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.111165\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.087228\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.036824\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.054785\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.090443\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.204712\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.023395\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.134119\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.063132\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.056782\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.109781\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.087048\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.191695\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.086027\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.157661\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.181755\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.013458\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.055547\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.041528\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.096730\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.083873\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.014300\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.053316\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.062135\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.022755\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.130602\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.134795\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.013696\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.013972\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.014713\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.034737\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.165551\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.123771\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.047570\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.094942\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.042969\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.078181\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.136411\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.140460\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.119989\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.075457\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.069743\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.006621\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.029732\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.190367\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.080473\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.033097\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.100619\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.082786\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.080262\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.023712\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.059688\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.074297\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.079179\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.076974\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.031126\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.067932\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.100701\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.050649\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.047168\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.144526\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.115541\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.167343\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.099213\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.136243\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.056840\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.084417\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.029404\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.086043\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.125714\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.037135\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.012440\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.088103\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.103345\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.021074\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.036329\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.197267\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.047356\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.032842\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.175686\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.015718\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.006144\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.002686\n",
            "\n",
            "Test set: Average loss: 0.0472, Accuracy: 9837/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.059799\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.036265\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.034226\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.102618\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.099674\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.014239\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.004838\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.076462\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.108025\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.055092\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.203007\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.079479\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.039432\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.021605\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.092305\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.077152\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.205171\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.026448\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.123572\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.082447\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.069397\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.044272\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.051742\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.272285\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.066318\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.105834\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.099139\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.016799\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.082030\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.092677\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.146596\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.064578\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.011429\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.061401\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.023023\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.017842\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.185855\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.064069\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.012422\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.015843\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.036176\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.022920\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.075816\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.064195\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.091297\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.043710\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.048860\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.124019\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.020155\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.050834\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.129038\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.108808\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.104899\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.014785\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.019891\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.129326\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.170295\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.018393\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.016188\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.097207\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.058532\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.014564\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.030769\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.089161\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.071208\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.053537\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.017807\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.099839\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.072739\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.072957\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.134602\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.112686\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.102541\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.129331\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.099176\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.069110\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.041312\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.083881\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.027289\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.073223\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.134394\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.077478\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.008842\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.087611\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.038606\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.044761\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.025781\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.160414\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.042601\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.021305\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.120390\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.006187\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.006528\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.012188\n",
            "\n",
            "Test set: Average loss: 0.0451, Accuracy: 9845/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.023528\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.032062\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.058833\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.065319\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.018929\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.025634\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.014285\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.060086\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.096129\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.094842\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.174860\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.076840\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.058631\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.039361\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.107706\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.024001\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.119461\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.021280\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.192324\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.064618\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.094809\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.022401\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.041193\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.108034\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.036783\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.047068\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.155773\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.020497\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.141787\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.049164\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.148011\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.066459\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.008955\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.047244\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.016508\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.017692\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.154131\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.078307\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.008551\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.015486\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.022194\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.026010\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.183065\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.139504\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.051575\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.027187\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.031092\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.038820\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.074240\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.031201\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.084371\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.049742\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.046524\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.019591\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.037584\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.122242\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.040695\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.005279\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.061285\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.125190\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.043471\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.003162\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.036885\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.067674\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.098843\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.039726\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.043108\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.067805\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.099459\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.100658\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.147728\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.155438\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.135422\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.090153\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.031219\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.040797\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.028389\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.085046\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.044370\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.056787\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.138562\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.047508\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.006838\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.096449\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.058156\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.085500\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.025755\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.083676\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.022954\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.041308\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.172978\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.012117\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.012077\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.002336\n",
            "\n",
            "Test set: Average loss: 0.0430, Accuracy: 9847/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.040912\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.020942\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.036788\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.094085\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.062938\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.033702\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.017620\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.074559\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.149335\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.077113\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.085536\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.086457\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.053604\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.012285\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.086154\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.068699\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.202772\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.029939\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.155922\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.074144\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.063844\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.046055\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.097620\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.103011\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.108622\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.147822\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.116283\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.025316\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.078865\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.032342\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.077520\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.078746\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.015279\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.053766\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.018702\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.019857\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.053086\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.210982\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.004043\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.024271\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.025725\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.024195\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.111468\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.117581\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.051646\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.037336\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.059330\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.043264\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.092432\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.063423\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.071178\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.147807\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.038965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-6a275a2fd2c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-6a275a2fd2c0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adadelta.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     86\u001b[0m                        \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                        \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                        weight_decay)\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madadelta\u001b[0;34m(params, grads, square_avgs, acc_deltas, lr, rho, eps, weight_decay)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0macc_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}