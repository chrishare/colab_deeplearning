{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2M2UmmYfrsiOfUkx8Ezx7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrishare/colab_deeplearning/blob/master/pytorch_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK2qy-3OEPuc",
        "outputId": "e5e08a2a-cf0c-4955-b1fa-bd20eac67b1a"
      },
      "source": [
        "# Some pytorch basics - first, a simple numpy-based regression problem\n",
        "# that attempts to produce learn a,b,c,d such that a + bx + cx^2 + dx^3\n",
        "# closely models sin(x) between -pi and +pi\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Create random input and output data\n",
        "# Get a row vector of linearly spaced numbers between -pi and +pi\n",
        "x = np.linspace(-math.pi, math.pi, 2000)\n",
        "# Get a row vector of sin(x_i) for each i in x\n",
        "y = np.sin(x)\n",
        "# Get the 'size'of x - which will be (2000,) - a 1-dim row vector of size 2000\n",
        "x.shape\n",
        "\n",
        "# Randomly initialize weights - get 4 individual floats that are \n",
        "a = np.random.randn()\n",
        "b = np.random.randn()\n",
        "c = np.random.randn()\n",
        "d = np.random.randn()\n",
        "\n",
        "# Set learning rate to be 1 / 10^6\n",
        "learning_rate = 1e-6\n",
        "\n",
        "# For 2000 iterations (epochs)\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    # y = a + b x + c x^2 + d x^3\n",
        "    # y is a 2000-el row vector based on the current weights\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss by calculating the square of every difference \n",
        "    # between the predication and actual answer (label), and summing\n",
        "    # So loss is a scalar of the magnitude of the loss on the whole dataset\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    # a = sum(2 * (y_pred - y_actual)) for each el in y\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    # b = sum(2 * (y_pred - y_actual) * x) for each el in y\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    # c = sum(2 * (y_pred - y_actual) * x^2) for each el in y\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    # d = sum(2 * (y_pred - y_actual) * x^3) for each el in y\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # Update weights - subtrack the gradient of the loss * learning rate\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 18.09889106562996\n",
            "199 15.105687454472273\n",
            "299 13.082607103470345\n",
            "399 11.713518430560395\n",
            "499 10.786040027133247\n",
            "599 10.157057297824208\n",
            "699 9.730041053359837\n",
            "799 9.439820602584472\n",
            "899 9.242353029135224\n",
            "999 9.10784353510595\n",
            "1099 9.0161151779539\n",
            "1199 8.95348972519423\n",
            "1299 8.910684511709542\n",
            "1399 8.881393000359349\n",
            "1499 8.86132584235104\n",
            "1599 8.847562388040501\n",
            "1699 8.838111690051994\n",
            "1799 8.831615010272614\n",
            "1899 8.827144003023928\n",
            "1999 8.824063654947992\n",
            "Result: y = 0.0022636170507037193 + 0.8582230470700756 x + -0.0003905116079966977 x^2 + -0.09354121965192531 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8puQWXjnEwhN"
      },
      "source": [
        "# Next, a pytorch example - though it uses CPU\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# Get a reference to the torch float datatype\n",
        "dtype = torch.float\n",
        "# Get a reference to the local CPU device - you can get a cuda GPU too\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# Create random input and output data, on the CPU using torch.float\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "# Get the ground truth sin(x) for every input x\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Randomly initialize weights, again, on CPU using torch.floar\n",
        "a = torch.randn((), device=device, dtype=dtype)\n",
        "b = torch.randn((), device=device, dtype=dtype)\n",
        "c = torch.randn((), device=device, dtype=dtype)\n",
        "d = torch.randn((), device=device, dtype=dtype)\n",
        "\n",
        "# The rest is as per numpy - it magically does the computation on the torch\n",
        "# device and types acquired when setting up the variables/tensors\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}